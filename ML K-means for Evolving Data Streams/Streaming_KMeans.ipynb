{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UFT4S6JqDGCx"},"outputs":[],"source":["# EficiencyExperiment = True\n","# SurrogateExperiment = True\n","\n","# MAIN = \"\" # MAIN DIRECTORY #"]},{"cell_type":"markdown","metadata":{"id":"4Xe42iYCDGCy"},"source":["### IMPORTS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFw3-1uXDGCz"},"outputs":[],"source":["#Requirements made with pipreqs or pipreqsnb\n","import os\n","import math\n","import scipy\n","import time\n","import pickle\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Patch\n","from sklearn.cluster import KMeans\n","from sklearn.cluster import MiniBatchKMeans\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import MDS\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vnDP5JSDGC0"},"outputs":[],"source":["## Here the directories for the results are generated, some folders are made on the directory were the folder Notebooks is ##\n","\n","def parent(path):\n","    return os.path.split(path)[0]\n","def sign(x):\n","    if x:\n","        return abs(x)/x\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","path = os.getcwd()\n","os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Results\"))) #Here results of the main experiments are stored\n","os.system(\"mkdir %s\"%(os.path.join(parent(path),\"Surrogate\"))) #Here results of the surrogate experiment are stored"]},{"cell_type":"markdown","metadata":{"id":"7TkJTnhKDGC2"},"source":["### Define error function, distance and label assignment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3c1ay4WlDGC3"},"outputs":[],"source":["## Some important functions and Distances are defined for experiments ##\n","\n","def compute_labs(x,centers): #Given a set of points and a set of centroids returns a list of labels\n","    labs = np.zeros(shape=(x.shape[0]))\n","    for i in range(x.shape[0]):\n","        labs[i]=np.argmin(np.linalg.norm((x[i,:]-centers),axis=1))\n","    return (labs)\n","\n","def distance(x,centers,compute_labs = False): #Computes a list of the distances from each point to each nearest centroid, if requested also returns a list of labels\n","    dis = np.zeros(shape=(x.shape[0]))\n","    if compute_labs:\n","        labs = np.zeros(shape=(x.shape[0]))\n","        for i in range(x.shape[0]):\n","            aux = np.linalg.norm((x[i,:]-centers),axis=1)\n","            dis[i] = aux.min()\n","            labs[i] = np.argmin(aux)\n","        return (dis,labs)\n","    else:\n","        for i in range(x.shape[0]):\n","            dis[i] = np.linalg.norm((x[i,:]-centers),axis=1).min()\n","        return (dis)\n","\n","def distance_labs(x,centers,labs): #This functions does the same but the labels are allready given, which speeds up computations\n","    dis = np.zeros(shape=(x.shape[0]))\n","    for i in range(x.shape[0]):\n","            dis[i] = np.linalg.norm((x[i,:]-centers[labs[i],:]))\n","    return (dis)\n","\n","def inertia(x,centers,weight = np.array([None]),labs = np.array([None])): #Returns the K-means error(weighted if weight is given) for a dataset and centroids\n","    if any(weight==None):\n","        weight=np.ones(shape=(x.shape[0]))\n","    if any(labs!=None):\n","        error = np.sum(weight*(distance_labs(x,centers,labs))**2)\n","    else:\n","        error = np.sum(weight*(distance(x,centers))**2)\n","    return (error/np.sum(weight))\n","\n","def print_and_log(log_file,text): #If wanted to log the experiment steps, this function prints a text to the log file and to the screen\n","    print(text,file=log_file)\n","    os.system(\"echo \"+str(text))\n","    \n","def max_order(a): #Helps debugging, returns the maximum order of magnitude(in base 10) of the values in an array\n","    return round(np.log(a.max()+1.0e-10)/np.log(10))"]},{"cell_type":"markdown","metadata":{"id":"0FpaTVwjDGC4"},"source":["### Read datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EQI9NoNmDGC5"},"outputs":[],"source":["## Here an object is defined in order to read datasets ##\n","\n","class Dataset:\n","    \n","    \"Data reader\"\n","    #File in .csv format, a single file can be given or a folder containing many datasets(with the same number of columns). Variables should be numerical, so in order to remove \n","    #class variables or any non interesting variables insert a list of indexes corresponding to the columns to be removed. If the data was already stored as a Python object, it can be stated with the last input variable\n","    \n","    def __init__(self,file = None,folder = None,class_index = [],header = \"infer\",is_python_object=False): \n","        \n","        if is_python_object:\n","            with open(file, 'rb') as input:\n","                self.data= pickle.load(input)\n","        else:\n","            if file:\n","                df = pd.read_csv(file,header = header) #Reads .csv file using panda's dataframe\n","                #df = df.fillna(axis = 1,method = \"bfill\") #If there are missing values uncomment this line, and use the desired filling method\n","                self.classes = df[df.columns[class_index]] #Creates the list of columns to be removed\n","                df.drop(labels = df.columns[class_index],axis = 1,inplace = True) #Remove the undesired columns\n","                self.names = list(df.columns) \n","                self.data = df.to_numpy()                \n","                del df #Delete dataframe\n","                \n","            #In this case the same procedure is conducted, but a list of .csv files are concatenated instead\n","            elif folder:\n","                df = pd.DataFrame()\n","                for f in os.listdir(folder):\n","                    if \".csv\" in f:\n","                        df = df.append(pd.read_csv(os.path.join(folder,f),header = header))\n","                #df = df.fillna(axis = 1,method = \"bfill\")\n","                self.classes = df[df.columns[class_index]]\n","                df.drop(labels = df.columns[class_index],axis = 1,inplace = True)\n","                self.names = list(df.columns)\n","                self.data = df.to_numpy()\n","                del df"]},{"cell_type":"markdown","metadata":{"id":"BlCZUYgrDGC6"},"source":["### Create Concept Drift"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jOxHwZQ9DGC6"},"outputs":[],"source":["## Here the function that generated controlled (1+epsilon)-drifts is given ##\n","\n","#The number of clusters must be given in order to compute a set of centroids. The number of concepts that must be generated can be given, if the e_list contains more than one value then the concepts are generated according \n","#to the list,but if e_list has a single value then n_concepts are generated with the epsilon value given on the list. The file/folder of the original dataset is given, and where the output streaming data is stored must be given.\n","#The tolerance of the heuristic procedure is set default to 0.05, and the class_index must be given as well as when a dataset was read.\n","\n","def Generate_concept(K = None, n_concepts = None, e_list= [None],file_in = None, file_out = None, tol = 0.05,class_index = []):\n","    \n","    if len(e_list)==1:\n","        aux = e_list\n","        e_list = [aux[0] for _ in range(n_concepts)]\n","        del aux\n","    else:\n","        n_concepts = len(e_list) #If e_list has more than one value then n_concept is the size of the list\n","\n","    #The dataset is read\n","    if os.path.isfile(file_in):\n","        data = Dataset(file = file_in,class_index = class_index).data\n","    else:\n","        data = Dataset(folder = file_in,class_index = class_index).data\n","    \n","    \n","    data = data[np.random.shuffle(np.arange(data.shape[0])),:][0] #Shuffle the data\n","    if data.shape[0]>100000: #For massive data, select only 100.000 data points maximum\n","        data = data[0:100000,:]\n","    \n","    init_tol = tol #The tolerance will increase as more iterations are run, in order to generate data in reasonable time\n","    cd_size = data.shape[0] #The concept drifts will be generated with the whole original dataset, then the number of points on each concept is the number of points from the original dataset\n","\n","    i = 0\n","    current_data = data[0:cd_size,:]\n","    out_data = current_data\n","    \n","    skm = KMeans(n_clusters = K,n_init = 1,init = \"k-means++\",max_iter = 100) #Define the K-means problem\n","    skm.fit(current_data) #Compute the centroids C_{1}\n","    \n","    for e in e_list:\n","        i+=1\n","        \n","        current_error = skm.inertia_/current_data.shape[0] #Compute E(X_{i-1},C_{i-1})\n","        current_centers = skm.cluster_centers_ #C_{i-1}\n","        labels = skm.labels_\n","        \n","        valid = False\n","        \n","        maximum_random = 50 #Maximum of random directions considered\n","        maximum_mag = 20 #Maximum number of magnitudes computed(alpha^*_i)\n","        j = 1\n","        tol = init_tol #Set tolerance to the initial value\n","        \n","        while j<maximum_random and not valid:\n","            \n","            random = np.random.random(size=(K,current_data.shape[1])) #Set a random direction for each cluster\n","            rand = np.divide(random,np.linalg.norm(random,axis = 1).reshape(random.shape[0],1)) #Normalize directions\n","            alpha = math.sqrt(e*current_error/(K*cd_size)) #Set initial value for alpha\n","            j+=1\n","            k = 0\n","            while k<maximum_mag and not valid:\n","                \n","                k+=1\n","                random = alpha*rand #Scale random directions\n","                traslated_data = current_data.copy()\n","                \n","                for lab in range(K):\n","                    traslated_data[np.where(labels==lab),:] = traslated_data[np.where(labels==lab),:] + random[lab,:] #Traslate each cluster\n","                \n","                new_error = inertia(traslated_data,current_centers) #Compute the new error\n","                print(\"Error ratio= %s\"%(new_error/current_error)) #For log\n","                per = ((new_error/current_error-1)-e)/e #Compute ratio with respect to the desired error\n","                print(per*100)\n","                \n","                if (abs(per)>1 and k>2): #If the error differs more than a 100% with respect to the desired one, compute other random directions\n","                    print(\"Change direction\")\n","                    break\n","                if abs(per)<tol: #If the difference is below the threshold, then it is a valid traslation and the loop is ended\n","                    valid = True                    \n","                else:\n","                    alpha-=(new_error/((1+e)*current_error)-1)*math.sqrt(e*current_error/(K*cd_size)) #If the difference is not below the threshold, update alpha \n","            tol+=0.01\n","        if valid:\n","            print(\"Success\")\n","            skm = KMeans(n_clusters = K,init = skm.cluster_centers_,max_iter = 100)\n","            skm.fit(traslated_data) #Compute new centroids C_{i}\n","            out_data = np.concatenate((out_data,traslated_data)) #Concatenate the generated data\n","            current_data = traslated_data.copy()\n","        else:\n","            print(\"Not succeded\")\n","        print(e)\n","    \n","    with open(file_out, 'wb') as output:\n","        pickle.dump((cd_size,out_data), output, pickle.HIGHEST_PROTOCOL) #Save the generated Streaming data as a python object\n","    \n","    #Remove data \n","    del data \n","    del current_data\n","    del traslated_data\n","    del out_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVl6Q7LdDGC7"},"outputs":[],"source":["## Here some dictionaries and lists are used to determine how to generate the streaming data for each dataset ##\n","\n","#A dict containing the class_indexes for each dataset\n","index_dict = {\"Urban\":[],\"Google\":[0],\"Postures\":[0,1],\"SUSY\":[0],\"Gas\":[0],\"Pulsar\":[8],\"MiceProtein\":[0,-4,-3,-2,-1],\"Frogs\":[0,-4,-3,-2,-1],\"Epilepsia\":[0,-1],\"Gesture\":[-2,-1]} \n","#A list of used number of clusters to generate different streaming data\n","k_list = [5,10,25,50]\n","#A dictionary with the empirical optimal values of K for each dataset\n","k_dict = {\"SUSY\":2,\"Pulsar\":2,\"Epilepsia\":5,\"Urban\":400,\"Frogs\":60, \"Google\": 10,\"MiceProtein\":10,\"Gesture\":10,\"Gas\":5}\n","#This boolean determines if the number of clusters is used from the list or using the dictionary\n","k_from_list = True\n","#The names of the dataset used to generate different streamming data\n","names = [\"Urban\",\"SUSY\",\"Pulsar\",\"Epilepsia\",\"Google\",\"Frogs\",\"Gesture\",\"Gas\"]#\"MiceProtein\"\n","#Dictionary containing the input file/folder of each dataset\n","direc_dict = {\"Urban\":\"urbanGB.txt\",\"Google\":\"google_review_ratings.csv\",\"Postures\":\"Postures.csv\",\"Frogs\":\"Frogs_MFCCs.csv\",\"SUSY\":\"SUSY.csv\",\"Gas\":\"gas-sensor-array-temperature-modulation\",\"Gesture\":\"gesture_phase_dataset\",\"Pulsar\":\"HTRU_2.csv\",\"MiceProtein\":\"MiceProtein.csv\",\"Epilepsia\":\"Epilepsia.csv\"}\n","#List of epsilon values used, for this experiment n concepts are generated with the same epsilon value, hence each value defined in this list determines different experiments\n","e_list = [0.5,1,2]\n","#Dictionary determining the string representation of the epsilon values used on the name when saving the output\n","e_dict = {0.5:\"05\",1:\"1\",2:\"2\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9DOu5RzZDGC8"},"outputs":[],"source":["generate = False #Whether to generate or not\n","\n","if generate:\n","    for k in k_list:\n","        for e in e_list:\n","            for name in names:\n","                if not k_from_list:\n","                    k = k_dict[name]\n","                Generate_concept(K = k, n_concepts = 9, e_list= [e],file_in = os.path.join(parent(path),\"Datasets\",direc_dict[name]), \n","                                 file_out = os.path.join(parent(path),\"Python Objects\",name+e_dict[e]+\"k=\"+str(k)), tol = 0.05,class_index = index_dict[name])\n","                print(\"Done with %s for K=%s and e=%s\\n\"%(name,k,e))\n","\n","#Thse lines below define isolated generations\n","#e_list = [0.5]#[0.1,0.2,0.5,1,2,10]\n","#Generate_concept(k = 13, n_concepts = 9, e_list= e_list,file_in = os.path.join(MAIN,\"Datasets/gas-sensor-array-temperature-modulation\"), file_out = os.path.join(MAIN,\"Python Objects/SimulatedData05\"), tol = 0.1)\n","#Generate_concept(k = 1, n_concepts = 1, e_list= e_list,file_in = os.path.join(MAIN,\"Datasets/gas-sensor-array-temperature-modulation\"), file_out = os.path.join(MAIN,\"Python Objects/SimulatedData1\"), tol = 0.06)\n","#Generate_concept(k = 13, n_concepts = 9, e_list= e_list,file_in = os.path.join(MAIN,\"Datasets/SUSY.csv\"), file_out = os.path.join(MAIN,\"Python Objects/AritzCDSUSY\"), tol = 0.05)"]},{"cell_type":"markdown","metadata":{"id":"EgrM0hVKDGC8"},"source":["### Stream generator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zO-2PWqDDGC8"},"outputs":[],"source":["## This Pyhton object generates an iterator that returns batches of data with controlled concept drift ##\n","\n","class StreamCD:\n","\n","    \"\"\"Iterator that returns batches of data\"\"\"\n","    \n","    #Input file with the streaming data(in our case a python object), determine the batch size and the desired period of each concept drift. \n","    #First batches file contains the original dataset and is used to burn out the stored batches before making measurements, tmax determines \n","    #how many batches untill the burn out. Class index is used again for the original dataset.\n","    \n","    def __init__(self,file = \"\",batch_size = None,cd_period = None,first_batches=\"\",tmax = 0,class_index = []):\n","        \n","        with open(os.path.join(file), 'rb') as input:\n","            \n","            (concept_size,data)=pickle.load(input) #Load streaming data with the concept drift size(original data size)\n","        \n","        if first_batches: #If stated, read first batches\n","            if os.path.isfile(first_batches):\n","                self.initial_data = Dataset(file = first_batches,class_index = class_index).data\n","            else:\n","                self.initial_data = Dataset(folder = first_batches,class_index = class_index).data\n","                \n","        self.tmax = tmax\n","        self.cd_size = concept_size\n","        self.data = data\n","        self.batch_size = batch_size\n","        self.max = data.shape[0]\n","        self.num = 0\n","        self.concept = 0\n","        self.cd_period = cd_period\n","        self.i = 0\n","        self.initial = True\n","        self.swap = True\n","        \n","        if self.cd_period and not self.batch_size: #If the period is stated but not the batch size then compute it such that the maximum number of data is extracted\n","            self.batch_size = self.cd_size//self.cd_period\n","            \n","        if self.batch_size*self.cd_period>self.cd_size: #The data is extracted in order without replacement, so we can not extract more data than we have\n","            print(\"Imposible\")\n","            raise StopIteration\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        \n","        cd = False\n","        if self.i>self.tmax and self.swap: #If tmax batches have already passed, swap to the streaming data with concept drifts\n","                self.initial = False\n","                self.num = 0\n","                self.i = 0\n","                self.swap = False\n","                \n","        if self.num+self.batch_size>self.initial_data.shape[0]: #If more batches are needed from the original data until the burn out, then restart from the beginning\n","            self.num = 0\n","                \n","        if self.initial: #Return batches from the original dataset\n","            batch = self.initial_data[self.num:self.num+self.batch_size,:]\n","            self.num+=self.batch_size\n","            self.i+=1\n","        \n","        if not self.initial: \n","            \n","            if self.cd_period:#If cd_period is stated, then return batches with concept drift each period\n","                if self.i == self.cd_period:\n","                    self.concept+=1\n","                    self.num = self.concept*self.cd_size\n","                    self.i = 0\n","                    cd = True\n","\n","                if self.num>=self.data.shape[0]: #If all batches have been returned then stop iteration\n","                    raise StopIteration\n","\n","                self.i+=1\n","                aux = self.data[self.concept*self.cd_size:(self.concept+1)*self.cd_size,:]\n","                np.random.seed(int(10000*time.time())%(2**31))\n","                batch = aux[np.random.choice([i for i in range(aux.shape[0])],self.batch_size,False),:] #Choose a batch randomly\n","                self.num+=self.batch_size\n","\n","            else:#If cd_period is not stated, then return as many batches as possible from each concept \n","\n","                if self.num//self.cd_size != self.concept:\n","                    self.concept = self.num//self.cd_size\n","                    cd = True\n","                last_index = (self.concept+1)*self.cd_size\n","\n","                if self.num>=self.data.shape[0]:\n","                    raise StopIteration\n","\n","                if self.num+2*self.batch_size>last_index-1:\n","                    batch = self.data[self.num:self.num+self.batch_size,:]\n","                    self.num+=last_index\n","\n","                else:\n","                    batch = self.data[self.num:self.num+self.batch_size,:]\n","                    self.num+=self.batch_size\n","        \n","        return batch,cd #Return a batch and whether a concept drift has occurred or not\n","        "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"P7pZkRlKDGC9"},"source":["### Streaming K MEans"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKRCspygDGC9"},"outputs":[],"source":["## Define some useful functions for the experiments ##\n","\n","def dis_init_kmplus(n,k): #Returns the number of computed distances when initializing with KM++\n","    return n*k*(k-1)/2-((k-1)**3)/3-((k-1)**2)/2-(k-1)/6\n","\n","def Mean(x,ax): #Computes the mean value through an axis(ax)\n","    x = x\n","    if x.shape[ax]:\n","        return np.sum(x,axis=ax)/x.shape[ax]\n","    else:\n","        return np.zeros(x.shape[ax-1])\n","    \n","def kplus(X, K,w = [False]): #Computes the KM++ initialization, returns a set of initial Centroids\n","    np.random.seed(2020)            #Fixing seeds, so that every initialization(UPC,ICB,HI and WKI) computes the same C^0\n","    C = [X[np.random.choice(range(X.shape[0])),:]]\n","    for k in range(1, K):\n","        if np.array(w).any():\n","            D2 = w*np.array([min([np.inner(c-x,c-x) for c in C]) for x in X])\n","        else:\n","            D2 = np.array([min([np.inner(c-x,c-x) for c in C]) for x in X])\n","        probs = D2/D2.sum()\n","        cumprobs = probs.cumsum()\n","        r = np.random.random()\n","        for j,p in enumerate(cumprobs):\n","            if r < p:\n","                i = j\n","                break\n","        C.append(X[i,:])\n","    return np.array(C)\n","\n","def count(labels,k): #Returns two lists, the first one with all the labels ordered, and the second one with how many of each label is in the input list labels(ordered)\n","    \n","    aux_u,aux_count = np.unique(labels,return_counts=True)\n","    if len(aux_u) == k:\n","        return aux_u,aux_count\n","    else: #If not every label is present, then 0s need to be added to the count list\n","        u = list(range(k))\n","        count = list(range(k))\n","        for lab in u:\n","            if lab in aux_u:\n","                count[lab] = aux_count[np.where(aux_u == lab)][0]\n","            else:\n","                count[lab] = 0\n","        return u,count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vEjOtUE-DGC9"},"outputs":[],"source":["## Object where the Streaming K-means is carried out, with different methods FSKM(with different initializations) and PSKM ##\n","\n","class SkMeans:\n","    \n","    #Takes an initial batch a executes KM over it, with k clusters. Afterwards FSKM with forget parameter rho or PSKM is executed. With a threshold given, maximum number\n","    #of stored batches is determined with rho, or viceversa. The method determines which initialization technique to use when FSKM is used, or whether to use PSKM. The first \n","    #initialization method is KM++ by default. If wanted to run in multiple threads the number of \n","    #used cores is determined by jobs(default 1). n_init determines if the initialization shall be repeated. mini_batch initialization ban also be used. The variable plot accepts \n","    #a list containing \"pca\" or \"mds\", which will plot a PCA or MDS 2D projection of the datapoints, cmap determines the colormap used for this plot.\n","    \n","    def __init__(self, initial_batch = None, k = 2, rho = None, thresh = None,maxBatch = None,method = None, \n","                 first_init = \"k-means++\",chain_length = 200,afkmc2 = False,jobs = None,n_init = 1,mini_batch_size = None, plot = [\"\"], cmap = \"hsv\"):\n","        \n","        ### Save important parameters\n","        self.k = k\n","        self.first_init = first_init\n","        self.jobs = jobs\n","        self.n_init = n_init\n","        self.mini_batch_size = mini_batch_size\n","        self.chain_length = chain_length\n","        self.afkmc2 = afkmc2\n","        self.method = method\n","        self.dis_init = [0]\n","        \n","        if maxBatch == None:\n","            self.maxBatch = math.ceil(math.log(thresh)/math.log(rho)) #If maxBatch was not given then calculate it\n","        elif rho == None:\n","            rho = thresh**(1./maxBatch) #If rho was not given then calculate it\n","            self.maxBatch = maxBatch\n","        self.rho = np.array([rho**t for t in range(self.maxBatch,-1,-1)])\n","        self.thresh = thresh\n","        self.batches = [initial_batch] #Initiate array of batches\n","        \n","        ### Choose the kmeans method\n","        if self.mini_batch_size:\n","            self.kmeans = MiniBatchKMeans(n_clusters = self.k,n_init = self.n_init,init = self.first_init,batch_size = self.mini_batch_size)\n","        elif self.first_init == \"kmc2\":\n","            self.kmeans = KMeans(n_clusters = self.k,n_init = self.n_init,init = kmc2(np.vstack(self.batches),\n","                            k = self.k, chain_length=self.chain_length, afkmc2=self.afkmc2), n_jobs = self.jobs)\n","            self.dis_init[-1]+= dis_init_kmc2(m = self.chain_length,k = self.k)\n","        else:\n","            init_centers = kplus(self.batches[-1],self.k)\n","            self.error_real_init = [inertia(x = self.batches[-1],centers = init_centers)]\n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","            self.dis_init[-1]+= dis_init_kmplus(n = self.batches[0].shape[0],k = self.k)\n","        \n","        ### Fit to first batch\n","        start = time.time()\n","        self.kmeans.fit(self.batches[0],\n","                        sample_weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])]))\n","        self.time = [time.time()-start]\n","        \n","        ### Save results\n","        self.centers = [self.kmeans.cluster_centers_]\n","        self.errors = [self.kmeans.inertia_]\n","        self.error_ratio = [0]\n","        self.t = [0]\n","        self.lloyd_iter = [self.kmeans.n_iter_]\n","        self.dis_lloyd = [self.kmeans.n_iter_*self.k*sum([batch.shape[0] for batch in self.batches])]\n","        self.dis = [self.dis_lloyd[0]+self.dis_init[0]]\n","        self.drift = [False]\n","        self.cardinalities = np.zeros(shape = (self.k))\n","        self.batch_card = np.zeros(shape = (self.k))\n","        self.mean = np.zeros(shape = (self.k,initial_batch.shape[1]))\n","        self.labs = [self.kmeans.labels_]\n","        self.n_instances = [np.vstack(self.batches).shape[0]]\n","        self.error_conver = [self.errors[-1]/self.n_instances[-1]]\n","        self.error_init = []\n","        self.error_real_conver = [self.kmeans.inertia_]\n","        self.all_batches = [initial_batch]\n","        \n","        #If requested plot PCA or MDS projections of the first batch\n","        if \"pca\" in plot:\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            self.pca = PCA(n_components = 2).fit(initial_batch)\n","            projection = self.pca.fit_transform(initial_batch)\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"PC2\",size=20)\n","            plt.xlabel(\"PC1\",size=20)\n","            plt.title(\"PCA projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"PCA_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","        \n","        if \"mds\" in plot:\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            self.mds = MDS(n_components = 2).fit(initial_batch)\n","            projection = self.mds.fit_transform(initial_batch)\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"X2\",size=20)\n","            plt.xlabel(\"X1\",size=20)\n","            plt.title(\"MDS projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"MDS_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","            \n","    #This inner function is used each time a new batch arrives, and clusters the new batch and previous ones with different methods\n","        \n","    def Kmeans(self,batch,concept_drift = False,forget_centers = False,p=None,cd_index = 1,do_plot = False, plot = [\"\"], cmap = \"hsv\", reset_projection = False,drift = False, hungaro = False,prev_centers = np.array([False])):\n","        \n","        \n","        ### Procedure removing or not previous information, prev_centers is given so that C^* is the same for every FSKM initialization method\n","        \n","        if prev_centers.any() and self.method != \"minimize_real\":\n","            self.centers[-1] = prev_centers\n","            self.labs[-1] = compute_labs(np.vstack(self.batches),self.centers[-1]) #Recompute labels for future calculations\n","        self.all_batches.append(batch)\n","        self.t.append(self.t[-1]+1)\n","        self.drift.append(concept_drift)\n","        self.dis_lloyd.append(0)\n","        self.dis_init.append(0)\n","        self.lloyd_iter.append(0)\n","        \n","        start = time.time() #t_0 to measure elapsed time\n","                \n","        ########### INITIALIZATION ################### \n","        \n","        ## HI initialization ##\n","        \n","        if self.method == \"weights\": \n","            \n","            w_prev = np.zeros((self.k))\n","            i = self.labs[-1].shape[0]\n","            t = 0\n","\n","            while t<len(self.batches):\n","                w = self.rho[-2]**(t+1)\n","                batch_size = self.batches[len(self.batches)-t-1].shape[0]\n","                labs = self.labs[-1][i-batch_size:i]\n","                i-=batch_size\n","                u,c = count(labs,self.k)\n","                for lab in range(self.k):\n","                    w_prev[lab]+=w*c[lab]                    \n","                t+=1\n","\n","            if hungaro:\n","                p = np.zeros((self.k,self.k))\n","                f = np.zeros((self.k,self.k))\n","                previous_cluster_p = w_prev\n","\n","                km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","                km.fit(batch)\n","                self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                #self.dis_init[-1]+=(len(self.batches)+1)*self.k**2\n","                u,new_cluster_p = count(km.labels_,k = self.k)\n","                \n","                for k in range(self.k):\n","                    for kp in range(self.k):\n","                        p[k,kp]=1/(1+new_cluster_p[kp]/(previous_cluster_p[k]+1.0e-10))\n","                        f[k,kp]+=(new_cluster_p[kp]*p[k,kp]*np.linalg.norm(km.cluster_centers_[kp]-self.centers[-1][k])**2)\n","\n","                a = scipy.optimize.linear_sum_assignment(f)\n","                assignment = a[1]\n","\n","                init_centers = np.zeros((self.k,batch.shape[1]))\n","                for k in range(self.k):\n","                    kp = assignment[k]\n","                    init_centers[k,:]=p[k,kp]*self.centers[-1][k]+(1-p[k,kp])*km.cluster_centers_[kp]\n","\n","            else:\n","                p = np.zeros((self.k,self.k))\n","                previous_cluster_p = np.sum(temp_cluster_sizes,axis = 0)\n","\n","                km = KMeans(n_clusters = self.k,init = self.centers[-1],n_jobs = self.jobs,max_iter = 1)\n","                km.fit(batch)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                u,new_cluster_p = count(km.labels_,k = self.k)\n","                for k in range(self.k):\n","                    kp = k\n","                    p[k,kp]=1/(1+new_cluster_p[kp]/(previous_cluster_p[k]+1.0e-10))\n","\n","                init_centers = np.zeros((self.k,batch.shape[1]))\n","                for k in range(self.k):\n","                    kp = k\n","                    init_centers[k,:]=p[k,kp]*self.centers[-1][k]+(1-p[k,kp])*km.cluster_centers_[kp]            \n","            \n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","        \n","        ## WKI initialization ##\n","        \n","        if self.method == \"centroid_kmeans\":\n"," \n","            w_prev = np.zeros((self.k))\n","            i = self.labs[-1].shape[0]\n","            t = 0\n","            while t<len(self.batches):\n","                w = self.rho[-2]**(t+1)\n","                batch_size = self.batches[len(self.batches)-t-1].shape[0]\n","                labs = self.labs[-1][i-batch_size:i]\n","                i-=batch_size\n","                u,c = count(labs,self.k)\n","                for lab in range(self.k):\n","                    w_prev[lab]+=w*c[lab]                    \n","                t+=1\n","            km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","            km.fit(batch)\n","            self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","            self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","            u,w_0 = count(km.labels_,k = self.k)\n","            center_km = KMeans(n_clusters = self.k,init = kplus(np.concatenate((self.centers[-1],km.cluster_centers_),axis = 0),self.k,w=np.concatenate((w_prev,w_0),axis = 0)),n_jobs = self.jobs)\n","            self.dis_init[-1]+=dis_init_kmplus(n = 2*self.k,k = self.k)\n","            center_km.fit(np.concatenate((self.centers[-1],km.cluster_centers_),axis = 0),sample_weight = np.concatenate((w_prev,w_0),axis = 0))\n","            init_centers = center_km.cluster_centers_\n","            \n","            self.dis_init[-1]+= (center_km.n_iter_*2*self.k**2)\n","            \n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","            \n","        ## Update stored batches, and compute indexes of the recent concept ##\n","        if len(self.batches) == self.maxBatch:\n","            del self.batches[0]\n","        self.batches.append(batch)\n","        self.labs[-1] = compute_labs(np.vstack(self.batches),self.centers[-1])\n","        real_index = [x for x in range(-cd_index,0,1)]\n","        \n","        ## PSKM algorithm ##\n","        \n","        if self.method == \"minimize_real\": \n","            if cd_index != 1:\n","                init_centers = self.centers[-1] #Minimizes real SKM error but initializes with previous centroids if a CD did not happen\n","                \n","            else:\n","                km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","                km.fit(batch)\n","                self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                init_centers = km.cluster_centers_\n","                self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","                self.dis_init[-1]+= dis_init_kmplus(n = batch.shape[0],k = self.k)\n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","            \n","        ## UPC initialization ##\n","            \n","        if self.method == \"prev_centers\": \n","            init_centers = self.centers[-1]\n","            self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)   \n","            \n","        ## ICB initialization ##\n","        \n","        if forget_centers: #If chosen to forget, then for each new batch novel initial centers are computed\n","            if self.mini_batch_size:\n","                self.kmeans = MiniBatchKMeans(n_clusters = self.k,n_init = self.n_init,init = self.first_init,batch_size = self.mini_batch_size)\n","                \n","            elif self.first_init == \"kmc2\":\n","                self.kmeans = KMeans(n_clusters = self.k,n_init = self.n_init,init = kmc2(np.vstack(self.batches),\n","                            k = self.k, chain_length=self.chain_length, afkmc2=self.afkmc2,\n","                            weights = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])])),\n","                            n_jobs = self.jobs)\n","                self.dis_init[-1]+= dis_init_kmc2(m = self.chain_length,k = self.k)\n","                \n","            else:\n","                km = KMeans(n_clusters = self.k,init = kplus(batch,self.k),n_jobs = self.jobs)\n","                km.fit(batch)\n","                self.dis_init[-1]+=dis_init_kmplus(n = batch.shape[0],k = self.k)\n","                self.dis_init[-1]+=km.n_iter_*self.k*batch.shape[0]\n","                init_centers = km.cluster_centers_\n","                self.kmeans = KMeans(n_clusters = self.k,init = init_centers,n_jobs = self.jobs)\n","                self.dis_init[-1]+= dis_init_kmplus(n = batch.shape[0],k = self.k)\n","        \n","        \n","        ### LLOYD\n","        ## Fit to the new data ##        \n","        \n","        if self.method == \"minimize_real\": #Lloyd over SKM error\n","            self.kmeans.fit(np.vstack(np.array(self.all_batches)[real_index,:]))\n","        \n","        else: #Lloyd over surrogate error\n","            self.kmeans.fit(np.vstack(self.batches),\n","                            sample_weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])]))\n","        \n","        self.time.append(time.time()-start) #Compute elapsed time\n","        \n","        self.error_real_init.append(inertia(x = np.vstack(np.array(self.all_batches)[real_index,:]), centers = init_centers)) #Initial SKM error\n","        self.error_init.append(inertia(x = np.vstack(self.batches),centers = init_centers,weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])]))) #Initial surrogate error\n","        \n","        ## Save results ##\n","        \n","        self.labs.append(self.kmeans.labels_)\n","        self.centers.append(self.kmeans.cluster_centers_)\n","        self.errors.append(self.kmeans.inertia_)\n","        self.error_ratio.append(self.errors[-1]/self.errors[-2])\n","        self.dis_lloyd[-1]+=(self.kmeans.n_iter_*self.k*np.vstack(self.batches).shape[0])\n","        self.dis.append(self.dis_lloyd[-1]+self.dis_init[-1])\n","        self.lloyd_iter[-1]+=(self.kmeans.n_iter_)\n","        self.n_instances.append(np.vstack(self.batches).shape[0])\n","        self.error_conver.append(inertia(x = np.vstack(self.batches),centers = self.centers[-1],weight = np.array([self.rho[i] for i in range(-len(self.batches),0,+1) for _ in range(self.batches[i].shape[0])])))\n","        self.error_real_conver.append(inertia(x = np.vstack(np.array(self.all_batches)[real_index,:]), centers = self.centers[-1]))\n","        \n","        if \"pca\" in plot and do_plot: #If stated plot projection\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            if reset_projection:\n","                self.pca = PCA(n_components = 2).fit(np.vstack(self.batches))\n","            projection = self.pca.fit_transform(np.vstack(self.batches))\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"PC2\",size=20)\n","            plt.xlabel(\"PC1\",size=20)\n","            if drift:\n","                plt.title(\"PCA projection(Drift occurred): t = \"+str(self.t[-1]+1),size=20)\n","            else:\n","                plt.title(\"PCA projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"PCA_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","        \n","        if \"mds\" in plot and do_plot:\n","            labels = self.labs[-1]\n","            labels+= 1\n","            labels = labels/np.max(labels)\n","            cm = matplotlib.cm.get_cmap(cmap)\n","            if reset_projection:\n","                self.mds = MDS(n_components = 2).fit(np.vstack(self.batches))\n","            projection = self.mds.fit_transform(np.vstack(self.batches))\n","            plt.scatter(x=projection[:,0],y=projection[:,1],c = cm(labels))\n","            plt.ylabel(\"X2\",size=20)\n","            plt.xlabel(\"X1\",size=20)\n","            if drift:\n","                plt.title(\"MDS projection(Drift occurred): t = \"+str(self.t[-1]+1),size=20)\n","            else:\n","                plt.title(\"MDS projection: t = \"+str(self.t[-1]+1),size=20)\n","            plt.savefig(os.path.join(os.getcwd(),\"Projections\",\"MDS_\"+str(self.t[-1]+1)+\".pdf\"),format=\"pdf\")\n","        \n","    def save(self,folder,name):\n","        \n","        ## Delete most heavy data in the object and save it as pyhton object ##\n","        \n","        del self.kmeans\n","        del self.batches\n","        del self.all_batches\n","        with open(os.path.join(folder,name), 'wb') as output:\n","            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n","        \n","    def __str__(self):\n","        \n","        ## Return a string containing important information about the experiment procedure(parameters) ##\n","        \n","        n=40\n","        if self.jobs:\n","            if self.jobs < 0:\n","                return \"\\n\".join([\"Streaming Kmeans:\\n\",\"-\"*n,\"Data dimension = %s\" % self.batches[0].shape[1],\n","                          \"Number of clusters(k) = %s\" % self.k,\"Number of jobs(paralelization) = All\",\"First initialization: %s\" % self.first_init,\"-\"*n,\n","                          \"Forget parameter: \"+\"rho\"+\"= \"+str(self.rho[-2]),\"Threshold: \"+\"epsilon\"+\"= \"+str(self.thresh),\n","                          \"Maximum number of batches stored = %s\" % self.maxBatch,\"Number of processed batches = %s\" %(self.t[-1]+1)])\n","            else:\n","                return \"\\n\".join([\"Streaming Kmeans:\\n\",\"-\"*n,\"Data dimension = %s\" % self.batches[0].shape[1],\n","                          \"Number of clusters(k) = %s\" % self.k,\"Number of jobs(paralelization) = %s\" % self.jobs,\"First initialization: %s\" % self.first_init,\"-\"*n,\n","                          \"Forget parameter: \"+\"rho\"+\"= \"+str(self.rho[-2]),\"Threshold: \"+\"epsilon\"+\"= \"+str(self.thresh),\n","                          \"Maximum number of batches stored = %s\" % self.maxBatch,\"Number of processed batches = %s\" %(self.t[-1]+1)])\n","        else:\n","            return \"\\n\".join([\"Streaming Kmeans:\\n\",\"-\"*n,\"Data dimension = %s\" % self.batches[0].shape[1],\n","                          \"Number of clusters(k) = %s\" % self.k,\"Number of jobs(paralelization) = 1\",\"First initialization: %s\" % self.first_init,\"-\"*n,\n","                          \"Forget parameter: \"+\"rho\"+\"= \"+str(self.rho[-2]),\"Threshold: \"+\"epsilon\"+\"= \"+str(self.thresh),\n","                          \"Maximum number of batches stored = %s\" % self.maxBatch,\"Number of processed batches = %s\" %(self.t[-1]+1)])\n","    \n","    def summary(self):\n","        ## Print a summary ##\n","        print(self)\n","    \n","        "]},{"cell_type":"markdown","metadata":{"id":"avNLTg1aDGC-"},"source":["### Boxplot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKla8eV1DGC_"},"outputs":[],"source":["## Define functions in order to plot results ##\n","\n","def adapt_data(data=np.array([]),cd_per = 8): #Takes a data(stored over time), and modulates it to the concept drift period and can be plotted in boxplot format\n","    \n","    new_data = np.zeros(shape = (data.shape[1]//cd_per,cd_per,data.shape[0]))\n","    maximum = (data.shape[1]//cd_per)*cd_per\n","    for T in range(data.shape[0]):\n","        for t in range(cd_per):\n","            i = t\n","            index = list()\n","            while i<maximum:\n","                index.append(i)\n","                i+= cd_per\n","            new_data[:,(t)%cd_per,T] = data[T,index]\n","    return new_data   \n","\n","#The data to be plotted is stored in dictionaries. Each experiment has a dictionary, being the keys the names of hyperparameters and values its values. Keys are also related to the measurements, \n","#and the values are lists of the measures(initial error,elapsed time,...). xaxis is a dictionary with a single key, being the string to be plotted in the x axis, and the values to be stated as \n","#labels for this axis. Folder where the output pdf files should be saved. plot is a list of strings, which contains what measurements we want to plot. cm is the colormap. identity helps to \n","#differentiate the output apart from hyperparameter values on the title, save parameter works similarly but for the name of the saved file. scale accepts a dictionary with measures as keys\n","#and desired scales as values(default linear). Alpha determines the relative position of label ticks, should not be modified. ratio_ref is used if ratio is in plot list. cd_per \n","#is the period in which a concept is stable. rho_list is the list of forget parameters used, digit_r determines to which digit is round up when used as string. showfliers determines whether to\n","#show outliers or not. figsize determines the figure size, and epsilon is the value of epsilon during the experiments.\n","\n","def kmBoxplot(data=[],xaxis={},folder=None,plot=[None],cm=\"hsv\",identity=\"\",save = \"\",scale=dict(),alpha=1,\n","              ratio_ref = \"Minimize real error\",cd_per = 8,rho_list = [8,16], digit_r = 3, showfliers = True, figsize = (9,6),epsilon = 0.1):\n","    \n","    n_data = len(data)\n","    if n_data%2==0:\n","        relative=[i for i in range(-int(n_data/2),0)]\n","        relative.extend([i for i in range(1,int(n_data/2)+1)])\n","    else:\n","        relative=[i-n_data//2 for i in range(n_data)]\n","        \n","    ## Titles and y axis labels for each measurement ##\n","    \n","    title_dic = {\"surrogate_real\":\"Real and surrogate error difference histogram\",\"time\":\"Elapsed time\",\"error_real_conver\":\"Real error in convergence\",\"error_real_init\":\"Real initialization error\",\"error_init\":\"Initialization error comparison\",\"ratio\":\"Error ratio\",\"error_conver\":\"Converged error comparison\",\"n_iter\":\"Number of iterations comparison\",\"n_dis\":\"Computed distances comparison\",\"n_dis_lloyd\":\"Computed distances during Lloyd\",\"n_dis_init\": \"Computed distances during initialization\"}\n","    ylab_dic = {\"surrogate_real\":\"Density\",\"time\":\"Elapsed time(s)\",\"error_real_conver\":\"Normalized error\",\"error_real_init\":\"Normalized error\",\"error_init\": \"Normalized error\",\"ratio\":\"Error ratio\",\"error_conver\":\"Normalized error\",\"n_iter\":\"N iter\",\"n_dis\":\"N distances\",\"n_dis_lloyd\":\"N distances\",\"n_dis_init\":\"N distances\"}\n","    \n","    ## Run through all requested plots ##\n","    \n","    for pl in plot:\n","        \n","        if pl in scale.keys():\n","            sc = scale[pl]\n","        else:\n","            sc = \"linear\"\n","        if pl == \"surrogate_real\" or pl == \"ratio\":\n","            continue\n","        \n","        for T in range(len(rho_list)):\n","            \n","            r = rho_list[T]\n","                \n","            fig = plt.figure(1, figsize=figsize)\n","            ax = fig.add_subplot(111) #Create an axes instance\n","            cmap = matplotlib.cm.get_cmap(cm)  #Create a colormap instance\n","\n","            b = [None for _ in range(len(data))] #Boxplot plots list\n","\n","            for i in range(len(data)):\n","                col = cmap(i/(len(data))) #Define color for each method\n","                pos1 = np.array([value for value in xaxis.values()][0]) #Compute positions for each boxplot\n","                pos = alpha*len(data)*pos1\n","                for j in range(len(pos)):\n","                    pos[j]=pos[j]+relative[i]\n","                box = adapt_data(data[i][pl],cd_per)[:,:,T]\n","\n","                ## Plot each boxplot ##\n","                b[i] = ax.boxplot(box,patch_artist=True,\n","                            positions=pos,boxprops=dict(facecolor=col,color=\"black\"),\n","                            whiskerprops=dict(color=col,linestyle=\"--\"),capprops=dict(color=\"black\"),medianprops=dict(color=\"black\"),\n","                            flierprops=dict(marker=\"o\",color=col,alpha=1),manage_ticks = False, showfliers = showfliers)\n","                for flier in b[i][\"fliers\"]:\n","                    flier.set_markerfacecolor(col)\n","                    \n","            ## Set axis parameters and text ##\n","            ax.set_xticks(alpha*len(data)*pos1)\n","            ax.set_xticklabels([str(x) for x in [value for value in xaxis.values()][0]])\n","            ax.get_yaxis().tick_left()\n","            ax.legend([box[\"boxes\"][0] for box in b], [ dat[\"name\"] for dat in data], loc='upper right')\n","            ax.set_yscale(sc)\n","            pos = [value for value in xaxis.values()][0]\n","            if identity!=\"\":\n","                ax.set_title(title_dic[pl]+\": \"+identity,fontsize=\"xx-large\")\n","            else:\n","                ax.set_title(title_dic[pl],fontsize=\"xx-large\")\n","            ax.set_ylabel(ylab_dic[pl])\n","            ax.set_xlabel([value for value in xaxis.keys()][0])\n","            \n","            ## Draw vertical lines to separate each index ##\n","            for i in range(len(pos1)-1):\n","                ax.axvline(x = alpha*len(data)*(pos1[i]+pos1[i+1])/2,linestyle = \"-.\",color = \"grey\",linewidth = 1)\n","\n","            ## Save plots as pdf ##\n","            if folder:\n","                if save:\n","                    fig.savefig(os.path.join(folder,pl,pl+save+\".pdf\"),format=\"pdf\")\n","                else:\n","                    fig.savefig(os.path.join(folder,pl,pl+identity+\".pdf\"),format=\"pdf\")\n","            else:\n","                fig.savefig(os.path.join(parent(path),\"Results\",pl,pl+identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\".pdf\"),format=\"pdf\")\n","            fig.clf()    \n","    \n","    if \"surrogate_real\" in plot: #This one is computes relative differences between surrogate and SKM error\n","        pl = \"surrogate_real\"\n","        for T in range(len(rho_list)):\n","            r = rho_list[T]\n","            data_list = np.array([])\n","            for i in range(len(data)):\n","                Data = abs(data[i][\"error_real_conver\"]-data[i][\"error_conver\"])/data[i][\"error_real_conver\"]\n","                Data = Data[T,:]\n","                data_list = np.concatenate((data_list,Data),axis = 0)\n","            if len(data_list)==0:\n","                continue\n","            fig = plt.figure(1, figsize=figsize)\n","            ax = fig.add_subplot(111)\n","            ax.hist(np.log(data_list)/np.log(10),color = \"teal\",bins = 100)\n","            ax.set_title(title_dic[pl]+identity,fontsize=\"xx-large\")\n","            ax.set_ylabel(ylab_dic[pl])\n","            ax.set_xlabel(r\"$\\frac{|E_T-E_{\\rho}|}{E_T}$\",fontsize=\"x-large\")\n","            if folder:\n","                fig.savefig(os.path.join(folder,pl,pl+identity+\".pdf\"),format=\"pdf\")\n","            else:\n","                fig.savefig(os.path.join(parent(path),\"Results\",pl,pl+identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\".pdf\"),format=\"pdf\")\n","            fig.clf()\n","        \n","    if \"ratio\" in plot: #Plots measurements in ratios compared to the one determined by ratio_ref\n","    \n","        pl = \"ratio\"\n","        k = np.where(np.array([ dat[\"name\"] for dat in data]) == ratio_ref)[0][0]\n","        ref = data.pop(k)\n","        \n","        for error in [\"error_conver\",\"error_init\",\"error_real_conver\",\"error_real_init\"]:\n","            for T in range(len(rho_list)):\n","                \n","                r = rho_list[T]\n","                fig = plt.figure(1, figsize=figsize)\n","\n","                ## Create an axes instance ##\n","                ax = fig.add_subplot(111)\n","                cmap = matplotlib.cm.get_cmap(cm)\n","\n","                b = [None for _ in range(len(data))]\n","                \n","                for i in range(len(data)):\n","                    col = cmap(i/(len(data)))\n","                    pos1 = np.array([value for value in xaxis.values()][0])\n","                    pos = alpha*len(data)*pos1\n","                    for j in range(len(pos)):\n","                        pos[j]=pos[j]+relative[i]\n","                    box = adapt_data((data[i][error]-ref[error])/ref[error],cd_per)[:,:,T]\n","                    b[i] = ax.boxplot(box,patch_artist=True,\n","                                positions=pos,boxprops=dict(facecolor=col,color=\"black\"),\n","                                whiskerprops=dict(color=col,linestyle=\"--\"),capprops=dict(color=\"black\"),medianprops=dict(color=\"black\"),\n","                                flierprops=dict(marker=\"o\",color=col,alpha=1),manage_ticks = False, showfliers = showfliers)\n","                    for flier in b[i][\"fliers\"]:\n","                        flier.set_markerfacecolor(col)\n","                ax.set_xticks(alpha*len(data)*pos1)\n","                ax.set_xticklabels([str(x) for x in [value for value in xaxis.values()][0]])\n","                ax.get_yaxis().tick_left()\n","                ax.legend([box[\"boxes\"][0] for box in b], [ dat[\"name\"] for dat in data if dat[\"name\"]!=ratio_ref], loc='lower right')\n","                ax.set_yscale(scale)\n","                for xv in alpha*len(data)*pos1:\n","\n","                    ax.axvline(x = xv,linestyle = \"-.\",color = \"grey\",linewidth = 1)\n","\n","                if identity!=\"\":\n","                    ax.set_title(title_dic[pl]+\"(\"+error+\"): \"+identity,fontsize=\"xx-large\")\n","                else:\n","                    ax.set_title(title_dic[pl]+\"(\"+error+\")\",fontsize=\"xx-large\")\n","                ax.set_ylabel(ylab_dic[pl])\n","                ax.set_xlabel([value for value in xaxis.keys()][0])\n","                if folder:\n","                    fig.savefig(os.path.join(folder,pl,pl+error+identity+\".pdf\"),format=\"pdf\")\n","                else:\n","                    fig.savefig(os.path.join(parent(path),\"Results\",pl,pl+error+identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\".pdf\"),format=\"pdf\")\n","                fig.clf()      \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q92txEDkDGC_"},"outputs":[],"source":["## This function plots the results of the experiment where the surrogate is compared to the SKM error ##\n","\n","#Data contains the measured differences between both error functions, up and low contain theoretical upper and lower bounds. \n","#Every other parameter work similar to the previous function, plus there is another parameter N which states the batch size of the experiment.\n","\n","def SurrogateBoxplot(data=[],up=[],low=[],xaxis={},folder=None,r = 0.1,epsilon=0.1,cm=\"hsv\",identity=\"\",scale=\"linear\",alpha=1,\n","                     showfliers = True, figsize = (16,9), N = 10,delta_list = []):\n","                \n","    identity = \"Simulated Data\"\n","    fig = plt.figure(1, figsize=figsize)\n","\n","    ## Create an axes instance ##\n","    ax = fig.add_subplot(111)\n","    cmap = matplotlib.cm.get_cmap(cm)\n","    col = cmap(0.4)\n","    pos1 = np.array([value for value in xaxis.values()][0])\n","    s = 10\n","    ## Plot boxplots ##\n","    b = ax.boxplot(data,patch_artist=True,\n","                positions=pos1,boxprops=dict(facecolor=col,color=\"black\"),\n","                whiskerprops=dict(color=col,linestyle=\"--\"),capprops=dict(color=\"black\"),medianprops=dict(color=\"black\"),\n","                flierprops=dict(marker=\"o\",color=col,alpha=1),manage_ticks = False, showfliers = showfliers)\n","    \n","    for flier in b[\"fliers\"]:\n","        flier.set_markerfacecolor(col)\n","    ax.tick_params(axis='both', which='major', labelsize=s)\n","    x = np.linspace(pos1[0],pos1[-1],100)\n","    gauss = [0 for i in range(up.shape[0])]\n","    gauss_label = [Patch(facecolor = \"b\",alpha = (i+1)/(up.shape[0]+1),label = \"%s\"%(int(100*(1-delta_list[i])))+\"\\% confidence interval\") for i in range(up.shape[0])]\n","    ## Interpolate lines for upper and lower bounds and fill the space between them ##\n","    for i in range(up.shape[0]):\n","        upper = up[i,:]\n","        alpha = (i+1)/(up.shape[0]+1)\n","        interpolate = scipy.interpolate.make_interp_spline(pos1, upper)\n","        interpolate_upper = interpolate(x)\n","        l1 = ax.plot(x,interpolate_upper,alpha=alpha, color='b')\n","        lower = low[i,:]\n","        interpolate = scipy.interpolate.make_interp_spline(pos1, lower)\n","        interpolate_lower = interpolate(x)\n","        l2 = ax.plot(x,interpolate_lower,alpha=alpha, color='b')\n","        gauss[i] = plt.fill(np.concatenate([x, x[::-1]]),\n","             np.concatenate([interpolate_upper,\n","                            (interpolate_lower)[::-1]]),\n","             alpha=alpha, fc='b', ec='None')\n","    \n","    ## Axis setup ##\n","    ax.set_xticks(pos1)\n","    ax.set_xticklabels([str(x) for x in [value for value in xaxis.values()][0]],fontsize=s)\n","    ax.get_yaxis().tick_left()\n","    legend_elements = [Patch(facecolor = col,label = \"Observations\")]\n","    legend_elements.extend(gauss_label)\n","    s = 60\n","    ## Use tex fonts ##\n","    plt.rcParams.update({\n","    \"text.usetex\": True,\n","    \"font.weight\":\"light\"\n","})\n","    font = {\n","    \"fontweight\":\"light\",\n","    \"fontsize\":s\n","}\n","    ax.legend(handles=legend_elements,loc='upper right',fontsize=35)\n","    ax.tick_params(axis='both', which='major', labelsize=25)\n","    ax.set_yscale(scale)\n","    ax.set_xlabel(r\"Number of batches since last drift, $T$\",fontdict=font)\n","    ax.set_ylabel(r\"$E_*-E_{\\rho}$\",fontdict=font)\n","    ax.set_ylim(-1500,7500) #Set limits of y axis, same limits for each every experiment for comparability\n","    if identity!=\"\":\n","        ax.set_title(r\"$\\rho$\"+\" = \"+str(round(r,3))+\", \"+r\"$N$\"+\" = \"+str(N),fontdict=font)\n","    else:\n","        ax.set_title(\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\", N = \"+str(N),fontsize=s)\n","    \n","    ## Save figure ##\n","    if folder:\n","        fig.savefig(os.path.join(folder,identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\", N = \"+str(N)+\".pdf\"),format=\"pdf\")\n","    else:\n","        fig.savefig(os.path.join(parent(path),\"Surrogate\",identity+\", \"+\"rho\"+\" = \"+str(round(r,3))+\", \"+\"epsilon\"+\" = \"+str(epsilon)+\", N = \"+str(N)+\".pdf\"),format=\"pdf\")\n","    fig.clf() "]}],"metadata":{"colab":{"name":"Streaming_KMeans.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}
